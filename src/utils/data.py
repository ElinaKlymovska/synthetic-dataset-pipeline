"""
–£—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞–Ω–∏–º–∏, –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏ —Ç–∞ –µ–∫—Å–ø–æ—Ä—Ç–æ–º.
"""

import json
import yaml
import csv
from pathlib import Path
from typing import Dict, Any, Union
from datetime import datetime

from .file import ensure_directory


# =============================================================================
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
# =============================================================================

def load_yaml_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î YAML –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é."""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f) or {}


def save_yaml_config(config: Dict[str, Any], config_path: Union[str, Path]) -> None:
    """–ó–±–µ—Ä—ñ–≥–∞—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –≤ YAML —Ñ–æ—Ä–º–∞—Ç."""
    ensure_directory(Path(config_path).parent)
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2, allow_unicode=True)


def load_json_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î JSON –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é."""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json_config(config: Dict[str, Any], config_path: Union[str, Path]) -> None:
    """–ó–±–µ—Ä—ñ–≥–∞—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç."""
    ensure_directory(Path(config_path).parent)
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


def merge_configs(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∑–ª–∏–≤–∞—î –¥–≤—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó."""
    result = base.copy()
    
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = merge_configs(result[key], value)
        else:
            result[key] = value
    
    return result


# =============================================================================
# –ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö
# =============================================================================

def export_dataset_json(
    metadata_dir: Union[str, Path], 
    output_path: Union[str, Path],
    include_images: bool = False
) -> Dict[str, Any]:
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î –¥–∞—Ç–∞—Å–µ—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç."""
    metadata_dir = Path(metadata_dir)
    output_path = Path(output_path)
    
    ensure_directory(output_path.parent)
    
    dataset = {
        "export_info": {
            "created_at": datetime.now().isoformat(),
            "version": "2.0",
            "include_images": include_images
        },
        "images": []
    }
    
    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –º–µ—Ç–∞–¥–∞–Ω—ñ
    for metadata_file in metadata_dir.glob("*.json"):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                
            # –î–æ–¥–∞—î–º–æ –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            dataset["images"].append(metadata)
            
        except Exception as e:
            print(f"Error reading {metadata_file}: {e}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    dataset["statistics"] = {
        "total_images": len(dataset["images"]),
        "successful_generations": len([img for img in dataset["images"] 
                                      if img.get("status", {}).get("generation_status") == "success"]),
    }
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)
    
    return dataset


def export_dataset_csv(metadata_dir: Union[str, Path], output_path: Union[str, Path]) -> None:
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î –¥–∞—Ç–∞—Å–µ—Ç –≤ CSV —Ñ–æ—Ä–º–∞—Ç."""
    metadata_dir = Path(metadata_dir)
    output_path = Path(output_path)
    
    ensure_directory(output_path.parent)
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–æ–ª–æ–Ω–∫–∏
    fieldnames = [
        "image_id", "timestamp", "source_image", "generated_image",
        "prompt_text", "negative_prompt", "model_version",
        "steps", "guidance_scale", "strength", "resolution",
        "quality_score", "pose_category", "outfit_category",
        "generation_cost", "generation_time", "file_size_mb"
    ]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for metadata_file in metadata_dir.glob("*.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                core_info = metadata.get("core_info", {})
                gen_params = metadata.get("generation_params", {})
                lora_info = metadata.get("lora_training", {})
                status = metadata.get("status", {})
                image_props = metadata.get("image_properties", {})
                
                row = {
                    "image_id": core_info.get("image_id", ""),
                    "timestamp": core_info.get("timestamp", ""),
                    "source_image": core_info.get("source_image", ""),
                    "generated_image": core_info.get("generated_image", ""),
                    "prompt_text": gen_params.get("prompt_text", ""),
                    "negative_prompt": gen_params.get("negative_prompt", ""),
                    "model_version": gen_params.get("model_version", ""),
                    "steps": gen_params.get("steps", ""),
                    "guidance_scale": gen_params.get("guidance_scale", ""),
                    "strength": gen_params.get("strength", ""),
                    "resolution": gen_params.get("resolution", ""),
                    "quality_score": lora_info.get("quality_score", ""),
                    "pose_category": lora_info.get("pose_category", ""),
                    "outfit_category": lora_info.get("outfit_category", ""),
                    "generation_cost": status.get("generation_cost", ""),
                    "generation_time": status.get("generation_time_seconds", ""),
                    "file_size_mb": round(image_props.get("file_size_bytes", 0) / (1024*1024), 2)
                }
                
                writer.writerow(row)
                
            except Exception as e:
                print(f"Error processing {metadata_file}: {e}")


# =============================================================================
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞ –∞–Ω–∞–ª—ñ–∑
# =============================================================================

def calculate_dataset_stats(metadata_dir: Union[str, Path]) -> Dict[str, Any]:
    """–û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç—É."""
    metadata_dir = Path(metadata_dir)
    
    stats = {
        "total_files": 0,
        "valid_metadata": 0,
        "generation_success_rate": 0.0,
        "average_quality_score": 0.0,
        "total_cost": 0.0,
        "average_generation_time": 0.0,
        "pose_distribution": {},
        "outfit_distribution": {},
        "model_usage": {},
        "resolution_distribution": {},
        "file_size_stats": {
            "min_mb": float('inf'),
            "max_mb": 0.0,
            "avg_mb": 0.0
        }
    }
    
    valid_metadatas = []
    
    for metadata_file in metadata_dir.glob("*.json"):
        stats["total_files"] += 1
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            valid_metadatas.append(metadata)
            stats["valid_metadata"] += 1
            
        except Exception:
            continue
    
    if not valid_metadatas:
        return stats
    
    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –≤–∞–ª—ñ–¥–Ω—ñ –º–µ—Ç–∞–¥–∞–Ω—ñ
    successful_generations = 0
    quality_scores = []
    costs = []
    generation_times = []
    file_sizes = []
    
    for metadata in valid_metadatas:
        # –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        status = metadata.get("status", {})
        if status.get("generation_status") == "success":
            successful_generations += 1
        
        # –Ø–∫—ñ—Å—Ç—å
        lora_info = metadata.get("lora_training", {})
        quality = lora_info.get("quality_score", 0)
        if quality > 0:
            quality_scores.append(quality)
        
        # –í–∞—Ä—Ç—ñ—Å—Ç—å
        cost = status.get("generation_cost", 0)
        if cost > 0:
            costs.append(cost)
        
        # –ß–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        gen_time = status.get("generation_time_seconds", 0)
        if gen_time > 0:
            generation_times.append(gen_time)
        
        # –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∑ —ñ –æ–¥—è–≥—É
        pose = lora_info.get("pose_category", "unknown")
        outfit = lora_info.get("outfit_category", "unknown")
        
        stats["pose_distribution"][pose] = stats["pose_distribution"].get(pose, 0) + 1
        stats["outfit_distribution"][outfit] = stats["outfit_distribution"].get(outfit, 0) + 1
        
        # –ú–æ–¥–µ–ª—å
        gen_params = metadata.get("generation_params", {})
        model = gen_params.get("model_version", "unknown")
        stats["model_usage"][model] = stats["model_usage"].get(model, 0) + 1
        
        # –†–æ–∑–¥—ñ–ª—å–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å
        resolution = gen_params.get("resolution", "unknown")
        stats["resolution_distribution"][resolution] = stats["resolution_distribution"].get(resolution, 0) + 1
        
        # –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É
        image_props = metadata.get("image_properties", {})
        file_size_mb = image_props.get("file_size_bytes", 0) / (1024 * 1024)
        if file_size_mb > 0:
            file_sizes.append(file_size_mb)
    
    # –û–±—á–∏—Å–ª—é—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats["generation_success_rate"] = (successful_generations / len(valid_metadatas)) * 100
    stats["average_quality_score"] = sum(quality_scores) / len(quality_scores) if quality_scores else 0
    stats["total_cost"] = sum(costs)
    stats["average_generation_time"] = sum(generation_times) / len(generation_times) if generation_times else 0
    
    if file_sizes:
        stats["file_size_stats"] = {
            "min_mb": round(min(file_sizes), 2),
            "max_mb": round(max(file_sizes), 2),
            "avg_mb": round(sum(file_sizes) / len(file_sizes), 2)
        }
    
    return stats


def generate_quality_report(stats: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä—É—î —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∑–≤—ñ—Ç –ø—Ä–æ —è–∫—ñ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—É."""
    report_lines = [
        "üéØ –ó–í–Ü–¢ –ü–†–û –Ø–ö–Ü–°–¢–¨ –î–ê–¢–ê–°–ï–¢–£",
        "=" * 50,
        "",
        f"üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:",
        f"   ‚Ä¢ –í—Å—å–æ–≥–æ —Ñ–∞–π–ª—ñ–≤: {stats['total_files']}",
        f"   ‚Ä¢ –í–∞–ª—ñ–¥–Ω–∏—Ö –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {stats['valid_metadata']}",
        f"   ‚Ä¢ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {stats['generation_success_rate']:.1f}%",
        f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—è —è–∫—ñ—Å—Ç—å: {stats['average_quality_score']:.2f}",
        f"   ‚Ä¢ –ó–∞–≥–∞–ª—å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å: ${stats['total_cost']:.2f}",
        f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {stats['average_generation_time']:.1f}—Å",
        "",
        f"üìÅ –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—ñ–≤:",
        f"   ‚Ä¢ –ú—ñ–Ω: {stats['file_size_stats']['min_mb']:.2f} MB",
        f"   ‚Ä¢ –ú–∞–∫—Å: {stats['file_size_stats']['max_mb']:.2f} MB", 
        f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—ñ–π: {stats['file_size_stats']['avg_mb']:.2f} MB",
        "",
        f"ü§∏ –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∑:",
    ]
    
    for pose, count in sorted(stats['pose_distribution'].items()):
        percentage = (count / stats['valid_metadata']) * 100
        report_lines.append(f"   ‚Ä¢ {pose}: {count} ({percentage:.1f}%)")
    
    report_lines.extend([
        "",
        f"üëó –†–æ–∑–ø–æ–¥—ñ–ª –æ–¥—è–≥—É:",
    ])
    
    for outfit, count in sorted(stats['outfit_distribution'].items()):
        percentage = (count / stats['valid_metadata']) * 100
        report_lines.append(f"   ‚Ä¢ {outfit}: {count} ({percentage:.1f}%)")
    
    return "\n".join(report_lines) 